# -*- coding: utf-8 -*-
"""ExtractLocationFrom Literature.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AreGI3DNSpSwywmlgERTFHicCjEuLFY2
"""

from google.colab import drive
drive.mount("/content/gdrive/")

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import spacy
from collections import Counter
from geopy.geocoders import Nominatim
import folium
from IPython.display import display, HTML
import warnings

warnings.filterwarnings("ignore")

# Load the TXT file
with open('/content/gdrive/MyDrive/Colab Notebooks/BCPolicy/savedrecs.txt', 'r', encoding='utf-8') as file:
    texts = file.readlines()
# Download NLTK data
nltk.download('stopwords')
nltk.download('punkt')

# Function to preprocess text
def preprocess_text(text):
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^A-Za-z0-9\s]+", "", text)
    text = text.lower()
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    return " ".join(tokens)

# Preprocess texts
processed_texts = [preprocess_text(text) for text in texts]

processed_texts

# Load SpaCy's pre-trained model
nlp = spacy.load("en_core_web_sm")

# Function to extract locations
def extract_locations(text):
    doc = nlp(text)
    return [ent.text for ent in doc.ents if ent.label_ == "GPE"]

# Extract locations from each processed text
locations = [extract_locations(text) for text in processed_texts]

# Function to get the main locations (up to two) for each paper
def main_locations(location_list):
    if location_list:
        most_common = Counter(location_list).most_common(2)
        return [loc[0] for loc in most_common]
    return []

# Get the main locations for each paper
main_locations_per_paper = [main_locations(location_list) for location_list in locations]

# Flatten the list of locations
flat_locations = [loc for sublist in main_locations_per_paper for loc in sublist]

location_counts = Counter(flat_locations)

# Sort the location counts by frequency
sorted_location_counts = location_counts.most_common()
#sorted_location_counts

#location_df = pd.DataFrame(sorted_location_counts, columns=['Location', 'Count'])

# Save the DataFrame to a CSV file
#csv_file_path = '/content/sorted_location_counts.csv'
#location_df.to_csv(csv_file_path, index=False)